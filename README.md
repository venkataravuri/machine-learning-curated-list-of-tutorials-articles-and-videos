# Artificial Intelligence / Machine Learning Notes

### What is hyper parameter?
They are simply the very "knobs" one "turns" when building/tuning a statistical learning model.  For example, in stochastic gradient descent, one of these can be the learning rate (or "step size" coefficient).  The reason for denoting one of these adjustable values in a machine learning algorithm with the prefix "hyper-" is to explicitly say that it is not a "model parameter" that is optimized/learned during a training phase (and instead is set by the human designer a priori, or adjusted automatically via external model mechanisms).  In the literature, hyper-parameters have also been referred to as "meta-parameters" and "free parameters".

Some examples of hyperparameters:
Number of leaves or depth of a tree
Number of latent factors in a matrix factorization
Learning rate (in many models)
Number of hidden layers in a deep neural network
Number of clusters in a k-means clustering

References
https://www.quora.com/What-are-hyperparameters-in-machine-learning


# Curated list of articles

## Natural Language Processing (NLP)

#### TFIDF Overview

https://www.quora.com/How-does-TfidfVectorizer-work-in-laymans-terms

#### Tutorials
https://www.youtube.com/watch?v=xvqsFTUsOmc

## Feature Engineering

### Categorical Features Encoding

Part 1: https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-66041f734512
Part 2: https://blog.myyellowroad.com/using-categorical-data-in-machine-learning-with-python-from-dummy-variables-to-deep-category-42fd0a43b009


